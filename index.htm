<html>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" /> 
  <script src="hidebib.js" type="text/javascript"></script>
  <title>Ruimin Ke - Homepage</title>
  <link href="https://fonts.googleapis.com/css?family=Titillium+Web:400,600,400italic,600italic,300,300italic" type="text/css" rel="stylesheet" />
  <link href="style.css" type="text/css" rel="stylesheet" />
  <body>
    <table cellpadding="10" width="1100" align="center" border="0">
      <tr>
        <td halign="center">
          <p align="center">
            <font size="7"> Ruimin "Roman" KE </font> </br>
            <font size="5"><i>How to pronounce?</i>  /'reimin/k&#601/</font>
          </p>
        </td>
      </tr>
      <tr>
        <td>
          <table cellpadding="10" width="100%" align="center" border="0">
            <tr>
              <td width="50%" valign="top">I am a tenure-track Assistant Professor at <a target="_blank" rel="noopener noreferrer" href="https://www.rpi.edu/">Rensselaer Polytechnic Institute</a> with the <a target="_blank" rel="noopener noreferrer" href="https://cee.rpi.edu/">Department of Civil and Environmental Engineering</a>.
                Our research group at RPI seeks talented and dedicated students to join our team. Additionally, we are open to collaborating with individuals and groups from both academia and industry who share our research interests.</br></br>
                I received my Ph.D. and M.S. degrees in Transportation Engineering at the <a target="_blank" rel="noopener noreferrer" href="https://www.washington.edu/">University of Washington</a>. I earned a M.S. degree in Computer Science from the <a target="_blank" rel="noopener noreferrer" href="https://illinois.edu/">University of Illinois at Urbana-Champaign</a>, and a B.E. degreee in Automation from <a target="_blank" rel="noopener noreferrer" href="https://www.tsinghua.edu.cn/en/">Tsinghua University</a>.
                Our research interest lies in Intelligent Transportation Systems and Automated Vehicles with focuses on Machine Learning, Sensor Fusion, and the Internet-of-Things applications. 
                I serve on the editorial board of four journals: <a target="_blank" rel="noopener noreferrer" href="https://www.nature.com/srep/">Nature-Scientific Report</a>, <a target="_blank" rel="noopener noreferrer" href="https://journals.sagepub.com/home/trr">Transportation Research Record</a>, <a target="_blank" rel="noopener noreferrer" href="https://www.springer.com/journal/42421">Data Science for Transportation</a>, and <a target="_blank" rel="noopener noreferrer" href="https://www.sciencedirect.com/journal/journal-of-air-transport-management">Journal of Air Transport Management</a>. I have been extremely privileged to receive multiple academic awards in recognition of my collaboratve research accomplishments, including the 2023 TRB Best Paper Award, 2022 Outstanding Paper Award from IEEE DTPI Conference, 2020-2021 COTA Best Dissertation Award, and Editor's Choice Paper Award from Journal of Transportation Engineering.
                <br /><br />In my free time, I have been a badminton player since eight years old and able to achieve significant milestones in the sport. I have won over 70 medals/trophies since 2001. I was crowned the junior champion of <a target="_blank" rel="noopener noreferrer" href="http://www.doc88.com/p-398368573587.html">Sichuan Province</a> (11 times) and <a target="_blank" rel="noopener noreferrer" href="http://sports.sina.com.cn/s/2004-07-28/0723320391s.shtml">Chengdu City</a> (15 times). Later, I joined the Tsinghua University Badminton Team, where I contined to excel. During my tenure, I clinched 9 titles of Beijing and the <a target="_blank" rel="noopener noreferrer" href="http://news.cupl.edu.cn/info/1054/19584.htm">national runner-up</a> for Tsinghua University. 
                I was the men's singles champion of Beijing (university students) for three consecutive years (<a target="_blank" rel="noopener noreferrer" href="http://v.youku.com/v_show/id_XMjYyMjA4NDAw.html">2011</a>, <a target="_blank" rel="noopener noreferrer" href="http://v.youku.com/v_show/id_XNDQyNzk2NzIw.html">2012</a>, <a target="_blank" rel="noopener noreferrer" href="https://www.tsinghua.edu.cn/info/1181/54935.htm">2013</a>). 
                After relocating to Seattle, I won 9 more trophies, such as the men's singles champion of <a target="_blank" rel="noopener noreferrer" href="http://students.washington.edu/bmtclub/">2015 Northwest Husky Badminton Open</a>, the men's singles runner-up of <a target="_blank" rel="noopener noreferrer" href="https://www.youtube.com/watch?v=KPBD_abbd6g">2016 WA State Badminton Open</a>, and the men's doubles runner-up of <a target="_blank" rel="noopener noreferrer" href="https://www.youtube.com/watch?v=E0kyC5zGu3Q">2018 WA State Badminton Closed</a>. 
                <br /><br /><a target="_blank" rel="noopener noreferrer" href="mailto:ker@rpi.edu">E-Mail</a> / <a target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/citations?user=7dTiQxUAAAAJ&hl=en">Google Scholar</a> / <a target="_blank" rel="noopener noreferrer" href="https://www.researchgate.net/profile/Ruimin_Ke">Research Gate</a> / <a target="_blank" rel="noopener noreferrer" href="https://www.linkedin.com/in/ruimin-ke-44794893/">LinkedIn</a> / <a target="_blank" rel="noopener noreferrer" href="https://github.com/superromanke">Github</a> / <a target="_blank" rel="noopener noreferrer" href="https://search.proquest.com/docview/2489315896?pq-origsite=gscholar&fromopenview=true">PhD Dissertation</a><br/></td>

              <td width="25%" valign="top">
                <img src="./icons/nyc_profile.jpg" style="border-style: none" width="100%" /></br>
              </td>
            </tr>
          </table>
        </td>
      </tr>
      <tr>
        <td>
          <heading>News</heading>
          <br />
          <ul>
          <li><b>2023/08</b>: A proposal I lead as the PI on Machine Learning in Transportation Education is recommended for funding by the National Science Foundation.
          <li><b>2023/08</b>: Our very own Talha Azfar did a great job in unveiling the outcomes of the USDOT C2SMARTER project on transportation digital twin. <a target="_blank" rel="noopener noreferrer" href="https://shop.elsevier.com/books/title/author/9780323961264?utm_source=google_ads&utm_medium=paid_search&utm_campaign=usdsa&gclid=Cj0KCQjw0IGnBhDUARIsAMwFDLky4jBbc_l6eus937eQsNHpFeOmkGIKtBMQVFHI4ewZw82mEf7poVcaAh_wEALw_wcB&gclsrc=aw.ds">Webinar Link</a>
          <li><b>2023/07</b>: I am happy to announce that I will join Rensselaer Polytechnic Institute as an Assistant Professor starting from August 2023.
          <li><b>2023/06</b>: I was appointed as the Chair for the Edge Computing Subcommittee of TRB Standing Committee on AI and Advanced Computing Application.
          <li><b>2023/04</b>: The first edition of my co-authored book <a target="_blank" rel="noopener noreferrer" href="https://shop.elsevier.com/books/title/author/9780323961264?utm_source=google_ads&utm_medium=paid_search&utm_campaign=usdsa&gclid=Cj0KCQjw0IGnBhDUARIsAMwFDLky4jBbc_l6eus937eQsNHpFeOmkGIKtBMQVFHI4ewZw82mEf7poVcaAh_wEALw_wcB&gclsrc=aw.ds">Machine Learning for Transportation Research and Applications</a> is published at Elsevier.
        </td>
      </tr>
      <tr>
        <td>
          <heading>Selected Publications</heading>
          <br />
          <br />
          <table cellpadding="10" width="100%" align="center" border="0">
            <tr>
              <td width="20%" valign="top">
                <img src="icons/iv2023.png" style="border-style: none" width="100%" />
              </td>
              <td width="80%" valign="top"><papertitle><a href="./pdfs/IV2023.pdf">Lightweight Edge Intelligence Empowered Near-crash Detection Towards Real-time Vehicle Event Logging</a></papertitle><br /><strong><b>Ruimin Ke</b></strong>, Zhiyong Cui, Yanlong Chen, Meixin Zhu, Hao Yang, Yifan Zhuang, Yinhai Wang<br />IEEE Transactions on Intelligence Vehicles, 2023<br /><div class="paper" id="IV2023"><a href="javascript:toggleblock('IV2023_abs')">abstract</a> / <a href="javascript:toggleblock('IV2023_bib')">bibtex</a> / <a target="_blank" rel="noopener noreferrer" href="https://ieeexplore.ieee.org/abstract/document/10036095">link</a> / <a target="_blank" rel="noopener noreferrer" href="https://www.youtube.com/watch?v=9NGo4Ef59i0">demo</a>
                <p align="justify"><i id="IV2023_abs">A major role of automated vehicles is that vehicles serve as mobile sensors for event detection and data collection, which support tactical automation in autonomous driving and post-analysis for traffic safety. However, most data collected during regular operations of vehicles are not of interest, while it costs a large amount of computation, communication, and storage resources on the cloud servers. Vehicular edge computing has emerged as a promising paradigm to balance these high costs in traditional cloud computing. But edge computers often have limited resources to support the high efficiency and intelligence of advanced vehicular functions. Motivated by the existing challenges and new concepts, this paper proposes and tests a lightweight edge intelligence framework for vehicle event detection and logging that runs in an event-based and real-time manner. Specifically, this paper takes vehicle-vehicle and vehicle-pedestrian near-crashes as the events of interest. The lightweight algorithm design of modeling the bounding boxes in object detection/tracking enables real-time edge intelligence onboard a vehicle; The event-based data logging mechanism eliminates redundant data onboard and integrates multi-source information for individual near-crash events. Comprehensive open-road tests on four transit vehicles have been conducted.</i></p>
                <bibtext xml:space="preserve" id="IV2023_bib">@article{ke2023lightweight, <br />title={Lightweight Edge Intelligence Empowered Near-crash Detection Towards Real-time Vehicle Event Logging},<br />author={Ke, Ruimin and Cui, Zhiyong and Chen, Yanlong and Zhu, Meixin and Yang, Hao and Zhuang, Yifan and Wang, Yinhai},<br />journal={IEEE Transactions on Intelligent Vehicles},<br />year={2023},<br />publisher={IEEE}<br />}<br /></bibtext></div><script language="JavaScript">hideblock('IV2023_bib');hideblock('IV2023_abs');</script></td>
            </tr>
            <tr>
              <td width="20%" valign="top">
                <img src="icons/dt2022.jpg" style="border-style: none" width="100%" />
              </td>
              <td width="80%" valign="top"><papertitle><a href="./pdfs/dt2022.pdf">Efficient Procedure of Building University Campus Models for Digital Twin Simulation</a></papertitle><br /> Talha Azfar, Jeffrey Weidner, Adeeba Raheem, <strong><b>Ruimin Ke</b></strong>, Kelvin Cheu<br />IEEE Journal of Radio Frequency Identification, 2022<br /><div class="paper" id="dt2022"><a href="javascript:toggleblock('dt2022_abs')">abstract</a> / <a href="javascript:toggleblock('dt2022_bib')">bibtex</a> / <a target="_blank" rel="noopener noreferrer" href="https://ieeexplore.ieee.org/abstract/document/9913679">link</a> / <a target="_blank" rel="noopener noreferrer" href="https://nyu.zoom.us/rec/play/0m3GOt7iWt-vNhStCJ-J1naYCwnGTi29B5cjJQCjmVbZeunkgxw4Bzfw_MqqwFwJR85CLu8r_Fhweh66.6rAC7u7TAZkBQsOC?continueMode=true&_x_zm_rtaid=ipupiJfQSz6GrYaOgJojxQ.1672900135494.974de250732fb4ac7be6138efebbbbf9&_x_zm_rhtaid=842">tutorial presentation</a>
                <p align="justify"><i id="dt2022_abs">Realistic digital geographical models of real-world locations are a necessary starting point for digital twin applications, especially for simulation and visualization. However, the visual fidelity of this first step is often neglected, since the effort involved is counterproductive to the main research focus. In this paper, we explore different tools and resources to assemble an efficient and convenient procedure to create a 3D digital model of a university campus that can support digital twin applications. Specifically, the terrain, buildings, and road network are combined into the CARLA project on Unreal Engine, enabling computer vision, traffic simulation, and autonomous driving experimentation.</i></p>
                <bibtext xml:space="preserve" id="dt2022_bib">@article{azfar2022efficient, <br />title={Efficient Procedure of Building University Campus Models for Digital Twin Simulation},<br />author={Azfar, Talha and Weidner, Jeffrey and Raheem, Adeeba and Ke, Ruimin and Cheu, Ruey Long},<br />journal={IEEE Journal of Radio Frequency Identification},<br />volume={6},<br />pages={769-773},<br />year={2022},<br />publisher={IEEE}<br />}<br /></bibtext></div><script language="JavaScript">hideblock('dt2022_bib');hideblock('dt2022_abs');</script></td>
            </tr>            
            <tr>
              <td width="20%" valign="top">
                <img src="icons/as2021.png" style="border-style: none" width="100%" />
              </td>
              <td width="80%" valign="top"><papertitle><a href="./pdfs/as2021.pdf">When Intelligent Transportation Systems Sensing Meets Edge Computing: Vision and Challenges</a></papertitle><br />Xuan Zhou, <strong><b>Ruimin Ke</b></strong>, Hao Yang, Chenxi Liu<br />Applied Sciences, 2021<br /><div class="paper" id="AS2021"><a href="javascript:toggleblock('AS2021_abs')">abstract</a> / <a href="javascript:toggleblock('AS2021_bib')">bibtex</a> / <a target="_blank" rel="noopener noreferrer" href="https://www.mdpi.com/2076-3417/11/20/9680">link</a><p align="justify"><i id="AS2021_abs">The widespread use of mobile devices and sensors has motivated data-driven applications that can leverage the power of big data to benefit many aspects of our daily life, such as health, transportation, economy, and environment. Under the context of smart city, intelligent transportation systems (ITS), as a main building block of modern cities, and edge computing (EC), as an emerging computing service that targets addressing the limitations of cloud computing, have attracted increasing attention in the research community in recent years. It is well believed that the application of EC in ITS will have considerable benefits to transportation systems regarding efficiency, safety, and sustainability. Despite the growing trend in ITS and EC research, a big gap in the existing literature is identified: the intersection between these two promising directions has been far from well explored. In this paper, we focus on a critical part of ITS, i.e., sensing, and conducting a review on the recent advances in ITS sensing and EC applications in this field. The key challenges in ITS sensing and future directions with the integration of edge computing are discussed.</i></p><bibtext xml:space="preserve" id="AS2021_bib">@article{zhou2021intelligent,, <br />title={When Intelligent Transportation Systems Sensing Meets Edge Computing: Vision and Challenges},<br />author={Zhou, Xuan and Ke, Ruimin and Yang, Hao and Liu, Chenxi},<br />journal={Applied Sciences},<br />volum={11},<br />number={20},<br />pages={9680},<br />year={2021},<br />publisher={Multidisciplinary Digital Publishing Institute}<br />}<br /></bibtext></div><script language="JavaScript">hideblock('AS2021_bib');hideblock('AS2021_abs');</script></td>
            </tr>
            <tr>
              <td width="20%" valign="top">
                <img src="icons/TITS2020parking.png" style="border-style: none" width="100%" />
              </td>
              <td width="80%" valign="top"><papertitle><a href="./pdfs/TITS2020parking.pdf">A Smart, Efficient, and Reliable Parking Surveillance System With Edge Artificial Intelligence on IoT Devices</a></papertitle><br /><strong><b>Ruimin Ke</b></strong>, Yifan Zhuang, Ziyuan Pu, Yinhai Wang<br />IEEE Transactions on Intelligent Transportation Systems, 2021<br /><div class="paper" id="TITS2020"><a href="javascript:toggleblock('TITS2020_abs')">abstract</a> / <a href="javascript:toggleblock('TITS2020_bib')">bibtex</a> / <a target="_blank" rel="noopener noreferrer" href="https://ieeexplore.ieee.org/document/9061155">link</a><p align="justify"><i id="TITS2020_abs">Cloud computing has been a main-stream computing service for years. Recently, with the rapid development in urbanization, massive video surveillance data are produced at an unprecedented speed. A traditional solution to deal with the big data would require a large amount of computing and storage resources. With the advances in Internet of things (IoT), artificial intelligence, and communication technologies, edge computing offers a new solution to the problem by processing all or part of the data locally at the edge of a surveillance system. In this study, we investigate the feasibility of using edge computing for smart parking surveillance tasks, specifically, parking occupancy detection using the real-time video feed. The system processing pipeline is carefully designed with the consideration of flexibility, online surveillance, data transmission, detection accuracy, and system reliability. It enables artificial intelligence at the edge by implementing an enhanced single shot multibox detector (SSD). A few more algorithms are developed either locally at the edge of the system or on the centralized data server targeting optimal system efficiency and accuracy. Thorough field tests were conducted in the Angle Lake parking garage for three months. The experimental results are promising that the final detection method achieves over 95% accuracy in real-world scenarios with high efficiency and reliability. The proposed smart parking surveillance system is a critical component of smart cities and can be a solid foundation for future applications in intelligent transportation systems.</i></p><bibtext xml:space="preserve" id="TITS2020_bib">@article{9061155, <br />title={A Smart, Efficient, and Reliable Parking Surveillance System With Edge Artificial Intelligence on IoT Devices},<br />author={Ke, Ruimin and Zhuang, Yifan and Pu, Ziyuan and Wang, Yinhai},<br />journal={IEEE Transactions on Intelligent Transportation Systems},<br />year={2020},<br />volum={},<br />number={},<br />pages={1-13}<br />}<br /></bibtext></div><script language="JavaScript">hideblock('TITS2020_bib');hideblock('TITS2020_abs');</script></td>
            </tr>
            <tr>
              <td width="20%" valign="top">
                <img src="icons/TRR2020.jpeg" style="border-style: none" width="100%" />
              </td>
              <td width="80%" valign="top"><papertitle><a href="./pdfs/TRR2020.pdf">Two-Stream Multi-Channel Convolutional Neural Network for Multi-Lane Traffic Speed Prediction Considering Traffic Volume Impact</a></papertitle><br /><strong><b>Ruimin Ke</b></strong>, Wan Li, Zhiyong Cui, Yinhai Wang<br />Transportation Research Record, 2020<br /><div class="paper" id="TRR2020"><a href="javascript:toggleblock('TRR2020_abs')">abstract</a> / <a href="javascript:toggleblock('TRR2020_bib')">bibtex</a> / <a target="_blank" rel="noopener noreferrer" href="https://journals.sagepub.com/doi/full/10.1177/0361198120911052">link</a> / <a target="_blank" rel="noopener noreferrer" href="http://uwdrive.net/STARLab">data (wsdot --> loopgroup data download)</a><p align="justify"><i id="TRR2020_abs">Traffic speed prediction is a critically important component of intelligent transportation systems. Recently, with the rapid development of deep learning and transportation data science, a growing body of new traffic speed prediction models have been designed that achieved high accuracy and large-scale prediction. However, existing studies have two major limitations. First, they predict aggregated traffic speed rather than lane-level traffic speed; second, most studies ignore the impact of other traffic flow parameters in speed prediction. To address these issues, the authors propose a two-stream multi-channel convolutional neural network (TM-CNN) model for multi-lane traffic speed prediction considering traffic volume impact. In this model, the authors first introduce a new data conversion method that converts raw traffic speed data and volume data into spatial�temporal multi-channel matrices. Then the authors carefully design a two-stream deep neural network to effectively learn the features and correlations between individual lanes, in the spatial�temporal dimensions, and between speed and volume. Accordingly, a new loss function that considers the volume impact in speed prediction is developed. A case study using 1-year data validates the TM-CNN model and demonstrates its superiority. This paper contributes to two research areas: (1) traffic speed prediction, and (2) multi-lane traffic flow study.</i></p><bibtext xml:space="preserve" id="TRR2020_bib">@article{ke2020TWO, <br />title={Two-Stream Multi-Channel Convolutional Neural Network for Multi-Lane Traffic Speed Prediction Considering Traffic Volume Impact},<br />author={Ke, Ruimin and Li, Wan and Cui, Zhiyong and Wang, Yinhai},<br />journal={Transportation Research Record},<br />pages={0361198120911052},<br />year={2020},<br />publisher={publisher={SAGE Publications Sage CA: Los Angeles, CA}<br />}<br /></bibtext></div><script language="JavaScript">hideblock('TRR2020_bib');hideblock('TRR2020_abs');</script></td>
            </tr>
	    <tr>
              <td width="20%" valign="top">
                <img src="icons/TRB2019uav.gif" style="border-style: none" width="100%" />
              </td>
              <td width="80%" valign="top"><papertitle><a href="./pdfs/IET2020uav.pdf">Advanced Framework for Microscopic and Lane-level Macroscopic Traffic Parameters Estimation from UAV Video</a></papertitle><br /><strong><b>Ruimin Ke</b></strong>, Shuo Feng, Zhiyong Cui, Yinhai Wang<br />IET Intelligent Transport Systems, 2020<br /><div class="paper" id="ke2019TRB"><a href="javascript:toggleblock('ke2019TRB_abs')">abstract</a> / <a href="javascript:toggleblock('ke2019TRB_bib')">bibtex</a> / <a target="_blank" rel="noopener noreferrer" href="https://digital-library.theiet.org/content/journals/10.1049/iet-its.2019.0463">link</a> / <a target="_blank" rel="noopener noreferrer" href="https://www.youtube.com/watch?v=qo4uYHAPMXk">video</a><p align="justify"><i id="ke2019TRB_abs">Unmanned aerial vehicle (UAV) is at the heart of modern traffic sensing research due to its advantages of low cost, high flexibility, and wide view range over traditional traffic sensors. Recently, increasing efforts in UAV-based traffic sensing have been made, and great progress has been achieved on the estimation of aggregated macroscopic traffic parameters. Compared to aggregated macroscopic traffic data, there has been extensive attention on higher-resolution traffic data such as microscopic traffic parameters and lane-level macroscopic traffic parameters since they can help deeply understand traffic patterns and individual vehicle behaviours. However, little existing research can automatically estimate microscopic traffic parameters and lane-level macroscopic traffic parameters using UAV videos with a moving background. In this study, an advanced framework is proposed to bridge the gap. Specifically, three functional modules consisting of multiple processing streams and the interconnections among them are carefully designed with the consideration of UAV video features and traffic flow characteristics. Experimental results on real-world UAV video data demonstrate promising performances of the framework in microscopic and lane-level macroscopic traffic parameters estimation. This research pushes off the boundaries of the applicability of UAVs and has an enormous potential to support advanced traffic sensing and management.</i></p><bibtext xml:space="preserve" id="ke2019TRB_bib">@article{ke2020advanced,<br />title={Advanced framework for microscopic and lane-level macroscopic traffic parameters estimation from UAV video},<br />author={Ke, Ruimin and Feng, Shuo and Cui, Zhiyong and Wang, Yinhai},<br />journal={IET Intelligent Transport Systems}<br />year = {2020}<br />publisher={IET}<br />}<br /></bibtext></div><script language="JavaScript">hideblock('ke2019TRB_bib');hideblock('ke2019TRB_abs');</script></td>
            </tr>
            <tr>
              <td width="20%" valign="top">
                <img src="icons/TITS2018uav.png" style="border-style: none" width="100%" />
              </td>
              <td width="80%" valign="top"><papertitle><a href="./pdfs/TITS2018uav.pdf">Real-Time Traffic Flow Parameter Estimation From UAV Video Based on Ensemble Classifier and Optical Flow</a></papertitle><br /><strong><b>Ruimin Ke</b></strong>, Zhibin Li, Jinjun Tang, Zewen Pan, Yinhai Wang<br />IEEE Transactions on Intelligent Transportation Systems, 2019<br /><div class="paper" id="TITS2018uav"><a href="javascript:toggleblock('TITS2018uav_abs')">abstract</a> / <a href="javascript:toggleblock('TITS2018uav_bib')">bibtex</a> / <a target="_blank" rel="noopener noreferrer" href="https://ieeexplore.ieee.org/abstract/document/8307444">link</a> / <a target="_blank" rel="noopener noreferrer" href="http://www.uwstarlab.org/publications.html#data">data</a><p align="justify"><i id="TITS2018uav_abs">Recently, the availability of unmanned aerial vehicle (UAV) opens up new opportunities for smart transportation applications, such as automatic traffic data collection. In such a trend, detecting vehicles and extracting traffic parameters from UAV video in a fast and accurate manner is becoming crucial in many prospective applications. However, from the methodological perspective, several limitations have to be addressed before the actual implementation of UAV. This paper proposes a new and complete analysis framework for traffic flow parameter estimation from UAV video. This framework addresses the well-concerned issues on UAV's irregular ego-motion, low estimation accuracy in dense traffic situation, and high computational complexity by designing and integrating four stages. In the first two stages an ensemble classifier (Haar cascade + convolutional neural network) is developed for vehicle detection, and in the last two stages a robust traffic flow parameter estimation method is developed based on optical flow and traffic flow theory. The proposed ensemble classifier is demonstrated to outperform the state-of-the-art vehicle detectors that designed for UAV-based vehicle detection. Traffic flow parameter estimations in both free flow and congested traffic conditions are evaluated, and the results turn out to be very encouraging. The dataset with 20,000 image samples used in this study is publicly accessible for benchmarking at http://www.uwstarlab.org/research.html.</i></p><bibtext xml:space="preserve" id="TITS2018uav_bib">@article{ke2018real, <br />title={Real-time traffic flow parameter estimation from UAV video based on ensemble classifier and optical flow},<br />author={Ke, Ruimin and Li, Zhibin and Tang, Jinjun and Pan, Zewen and Wang, Yinhai}, <br />journal={IEEE Transactions on Intelligent Transportation Systems},<br />year={2018},<br />volume={20}, <br />number={1},<br />pages={54-64},<br />publisher={IEEE}}<br /></bibtext></div><script language="JavaScript">hideblock('TITS2018uav_bib');hideblock('TITS2018uav_abs');</script></td>
            </tr>
            <tr>
              <td width="20%" valign="top">
                <img src="icons/ke2018JTE.jpg" style="border-style: none" width="100%" />
              </td>
              <td width="80%" valign="top"><papertitle><a href="./pdfs/JTE2018Wavelet.pdf">New Framework for Automatic Identification and Quantification of Freeway Bottlenecks Based on Wavelet Analysis</a></papertitle><br /><strong><b>Ruimin Ke</b></strong>, Ziqiang Zeng, Ziyuan Pu, Yinhai Wang<br />Journal of Transportation Engineering, Part A: Systems, 2018<br /><u>(Featured in the Editor's Choice Section of the journal)</u><br /><div class="paper" id="ke2018JTE"><a href="javascript:toggleblock('ke2018JTE_abs')">abstract</a> / <a href="javascript:toggleblock('ke2018JTE_bib')">bibtex</a> / <a target="_blank" rel="noopener noreferrer" href="https://ascelibrary.org/doi/abs/10.1061/JTEPBS.0000168">link</a><p align="justify"><i id="ke2018JTE_abs">As the amount of traffic congestion continues to grow, pinpointing freeway bottleneck locations and quantifying their impacts are crucial activities for traffic management and control. Among the previous bottleneck identification methods, limitations still exist. The first key limitation is that they cannot determine precise breakdown durations at a bottleneck in an objective manner. Second, the input data often needs to be aggregated in an effort to ensure better robustness to noise, which will significantly reduce the time resolution. Wavelet transform, as a powerful and efficient data-processing tool, has already been implemented in some transportation application scenarios to much benefit. However, there is still a wide gap between existing preliminary explorations of wavelet analysis in transportation research and a completely automatic bottleneck identification framework. This paper addresses several key issues in existing bottleneck identification approaches and also fills a gap in transportation-related wavelet applications. The experimental results demonstrate that the proposed method is able to locate the most severe bottlenecks and comprehensively quantify their impacts.</i></p><bibtext xml:space="preserve" id="ke2018JTE_bib">@article{doi:10.1061/JTEPBS.0000168,<br />author = {Ruimin Ke  and Ziqiang Zeng  and Ziyuan Pu  and Yinhai Wang },<br />title = {New Framework for Automatic Identification and Quantification of Freeway Bottlenecks Based on Wavelet Analysis},<br />journal = {Journal of Transportation Engineering, Part A: Systems},<br />volume = {144},<br />number = {9},<br />pages = {04018044},<br />year = {2018},<br />doi = {10.1061/JTEPBS.0000168}<br />}<br /></bibtext></div><script language="JavaScript">hideblock('ke2018JTE_bib');hideblock('ke2018JTE_abs');</script></td>
            </tr>
            <tr>
              <td width="20%" valign="top">
                <img src="icons/cvpr2017.jpg" style="border-style: none" width="100%" />
              </td>
              <td width="80%" valign="top"><papertitle><a href="./pdfs/cvpr2017.pdf"> A Cost-effective Framework for Automated Vehicle-pedestrian Near-miss Detection through Onboard Monocular Vision </a></papertitle><br /><strong><b>Ruimin Ke</b></strong>, Jerome Lutin, Jerry Spears, Yinhai Wang<br />Computer Vision and Pattern Recognition (CVPR), 2017<br /><div class="paper" id="cvpr2017"><a href="javascript:toggleblock('cvpr2017_abs')">abstract</a> / <a href="javascript:toggleblock('cvpr2017_bib')">bibtex</a> / <a target="_blank" rel="noopener noreferrer" href="https://ieeexplore.ieee.org/document/8014858">link</a> / <a target="_blank" rel="noopener noreferrer" href="https://www.youtube.com/watch?v=LPRWDhKdOiM">video</a> / <a target="_blank" rel="noopener noreferrer" href="http://depts.washington.edu/pactrans/pactrans-student-research-presents-at-ieee-cvpr-conference/">media coverage</a><p align="justify"><i id="cvpr2017_abs">Onboard monocular cameras have been widely deployed in both public transit and personal vehicles. Obtaining vehicle-pedestrian near-miss event data from onboard monocular vision systems may be cost-effective compared with onboard multiple-sensor systems or traffic surveillance videos. But extracting near-misses from onboard monocular vision is challenging and little work has been published. This paper fills the gap by developing a framework to automatically detect vehicle-pedestrian near-misses through onboard monocular vision. The proposed framework can estimate depth and real-world motion information through monocular vision with a moving video background. The experimental results based on processing over 30-hours video data demonstrate the ability of the system to capture near-misses by comparison with the events logged by the Rosco/MobilEye Shield+ system which includes four cameras working cooperatively. The detection overlap rate reaches over 90% with the thresholds properly set. </i></p><bibtext xml:space="preserve" id="cvpr2017_bib">@INPROCEEDINGS{8014858, <br />    author={R. Ke and J. Lutin and J. Spears and Y. Wang}, <br />    booktitle={2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)}, <br />    title={A Cost-Effective Framework for Automated Vehicle-Pedestrian Near-Miss Detection Through Onboard Monocular Vision},<br />    year={2017},<br />    pages={898-905},<br />   doi={10.1109/CVPRW.2017.124}, <br />    ISSN={2160-7516},<br />    month={July},}<br /></bibtext></div><script language="JavaScript">hideblock('cvpr2017_bib');hideblock('cvpr2017_abs');</script></td>			  
            </tr>
            <tr>
              <td width="20%" valign="top">
                <img src="icons/TITS2017uav.png" style="border-style: none" width="100%" />
              </td>
              <td width="80%" valign="top"><papertitle><a href="./pdfs/TITS2017uav.pdf"> Real-Time Bidirectional Traffic Flow Parameter Estimation From Aerial Videos </a></papertitle><br /><strong><b>Ruimin Ke</b></strong>, Zhibin Li, Sung Kim, John Ash, Yinhai Wang*<br />IEEE Transactions on Intelligent Transportation Systems, 2017<br /><div class="paper" id="TITS2017uav"><a href="javascript:toggleblock('TITS2017uav_abs')">abstract</a> / <a href="javascript:toggleblock('TITS2017uav_bib')">bibtex</a> / <a target="_blank" rel="noopener noreferrer" href="https://ieeexplore.ieee.org/abstract/document/7546916">link</a> / <a target="_blank" rel="noopener noreferrer" href="https://www.youtube.com/watch?v=m6ZEB_sqBek">demo</a><p align="justify"><i id="TITS2017uav_abs"> Unmanned aerial vehicles (UAVs) are gaining popularity in traffic monitoring due to their low cost, high flexibility, and wide view range. Traffic flow parameters such as speed, density, and volume extracted from UAV-based traffic videos are critical for traffic state estimation and traffic control and have recently received much attention from researchers. However, different from stationary surveillance videos, the camera platforms move with UAVs, and the background motion in aerial videos makes it very challenging to process for data extraction. To address this problem, a novel framework for real-time traffic flow parameter estimation from aerial videos is proposed. The proposed system identifies the directions of traffic streams and extracts traffic flow parameters of each traffic stream separately. Our method incorporates four steps that make use of the Kanade-Lucas-Tomasi (KLT) tracker, k-means clustering, connected graphs, and traffic flow theory. The KLT tracker and k-means clustering are used for interest-point-based motion analysis; then, four constraints are proposed to further determine the connectivity of interest points belonging to one traffic stream cluster. Finally, the average speed of a traffic stream as well as density and volume can be estimated using outputs from previous steps and reference markings. Our method was tested on five videos taken in very different scenarios. The experimental results show that in our case studies, the proposed method achieves about 96% and 87% accuracy in estimating average traffic stream speed and vehicle count, respectively. The method also achieves a fast processing speed that enables real-time traffic information estimation. </i></p><bibtext xml:space="preserve" id="TITS2017uav_bib"> @ARTICLE{7546916, <br /> author={R. Ke and Z. Li and S. Kim and J. Ash and Z. Cui and Y. Wang}, <br />journal={IEEE Transactions on Intelligent Transportation Systems}, <br /> title={Real-Time Bidirectional Traffic Flow Parameter Estimation From Aerial Videos},<br />year={2018},<br />volume={18}, <br />number={4},<br />pages={890-901},<br /> doi={10.1109/TITS.2016.2595526},<br />ISSN={1524-9050},<br />month={April},}<br /></bibtext></div><script language="JavaScript">hideblock('TITS2017uav_bib');hideblock('TITS2017uav_abs');</script></td>			  
            </tr>
            <tr>
              <td width="20%" valign="top">
                <img src="icons/isc22017.png" style="border-style: none" width="100%" />
              </td>
              <td width="80%" valign="top"><papertitle><a href="./pdfs/isc22017.pdf"> Roadway surveillance video camera calibration using standard shipping container </a></papertitle><br /><strong><b>Ruimin Ke</b></strong>, Zewen Pan, Ziyuan Pu, Yinhai Wang<br />IEEE International Smart Cities Conference, 2017<br /><div class="paper" id="Isc22017"><a href="javascript:toggleblock('Isc22017_abs')">abstract</a> / <a href="javascript:toggleblock('Isc22017_bib')">bibtex</a> / <a target="_blank" rel="noopener noreferrer" href=" https://ieeexplore.ieee.org/abstract/document/8090811">link</a><p align="justify"><i id="Isc22017_abs"> Surveillance video cameras have been increasingly deployed on roadway networks providing important support for roadway management. While the information-rich video images are a valuable source of traffic data, these surveillance video cameras are typically designed for manual observation of roadway conditions and are not for automatic traffic data collection. The benefits of turning these surveillance cameras into data collection cameras are obvious, but collecting traffic data would normally require the development of a cost-effective method to efficiently and accurately calibrate surveillance video cameras. This paper proposes such a robust and efficient method that calibrates surveillance video cameras using standard shipping container as the reference object. The traditional camera calibration model can be simplified and camera parameters can be recovered with precise mathematical derivation. After solving for all the camera parameters, the 3D object world coordinates can be reconstructed from 2D image coordinates, thus enabling the collection of a variety of traffic data using surveillance video camera data. </i></p><bibtext xml:space="preserve" id="Isc22017_bib"> @INPROCEEDINGS{8090811, <br /> author={R. Ke and Z. Pan and Z. Pu and Y. Wang},<br /> booktitle={2017 International Smart Cities Conference (ISC2)}, <br /> title={Roadway surveillance video camera calibration using standard shipping container},<br />year={2017},<br />volume={}, <br />number={},<br />pages={1-6},<br /> doi={10.1109/ISC2.2017.8090811},<br />ISSN={},<br />month={Sept},}<br /></bibtext></div><script language="JavaScript">hideblock('Isc22017_bib');hideblock('Isc22017_abs');</script></td>			  
            </tr>
            <tr>
              <td width="20%" valign="top">
                <img src="icons/isc22015.png" style="border-style: none" width="100%" />
              </td>
              <td width="80%" valign="top"><papertitle><a href="./pdfs/isc22015.pdf"> Motion-vector clustering for traffic speed detection from UAV video </a></papertitle><br /><strong><b>Ruimin Ke</b></strong>, Sung Kim, Zhibin Li, Yinhai Wang<br />IEEE First International Smart Cities Conference, 2015<br /><div class="paper" id="Isc22015"><a href="javascript:toggleblock('Isc22015_abs')">abstract</a> / <a href="javascript:toggleblock('Isc22015_bib')">bibtex</a> / <a target="_blank" rel="noopener noreferrer" href=" https://ieeexplore.ieee.org/document/7366230">link</a><p align="justify"><i id="Isc22015_abs"> A novel method for detecting the average speed of traffic from non-stationary aerial video is presented. The method first extracts interest points from a pair of frames and performs interest point tracking with an optical flow algorithm. The output of the optical flow is a set of motion vectors which are k-means clustered in velocity space. The centers of the clusters correspond to the average velocities of traffic and the background, and are used to determine the speed of traffic relative to the background. The proposed method is tested on a 70-frame test sequence of UAV aerial video, and achieves an average error for speed estimates of less than 12%.</i></p><bibtext xml:space="preserve" id="Isc22015_bib"> @INPROCEEDINGS{7366230, <br />author={R. Ke and S. Kim and Z. Li and Y. Wang}, <br /> booktitle={2015 IEEE First International Smart Cities Conference (ISC2)}, <br /> title={Motion-vector clustering for traffic speed detection from UAV video},<br />year={2015},<br />volume={}, <br />number={},<br />pages={1-5},<br /> doi={10.1109/ISC2.2015.7366230},<br />ISSN={},<br />month={Oct},}<br /></bibtext></div><script language="JavaScript">hideblock('Isc22015_bib');hideblock('Isc22015_abs');</script></td>			  
            </tr>
            <tr>
            <tr>
              <td width="20%" valign="top">
                <img src="icons/esa2018.png" style="border-style: none" width="100%" />
              </td>
              <td width="80%" valign="top"><papertitle><a href="./pdfs/esa2018.pdf">Lane-changes prediction based on adaptive fuzzy neural network </a></papertitle><br />Jinjun Tang, Fang Liu, Wenhui Zhang, <strong><b>Ruimin Ke</b></strong>, Yajie Zou<br />Expert Systems with Applications, 2018<br /><div class="paper" id="Esa2018"><a href="javascript:toggleblock('Esa2018_abs')">abstract</a> / <a href="javascript:toggleblock('Esa2018_bib')">bibtex</a> / <a target="_blank" rel="noopener noreferrer" href="https://www.sciencedirect.com/science/article/pii/S0957417417306255 ">link</a><p align="justify"><i id="Esa2018_abs"> Lane changing maneuver is one of the most important driving behaviors. Unreasonable lane changes can cause serious collisions and consequent traffic delays. High precision prediction of lane changing intent is helpful for improving driving safety. In this study, by fusing information from vehicle sensors, a lane changing predictor based on Adaptive Fuzzy Neural Network (AFFN) is proposed to predict steering angles. The prediction model includes two parts: fuzzy neural network based on Takagi–Sugeno fuzzy inference, in which an improved Least Squares Estimator (LSE) is adopt to optimize parameters; adaptive learning algorithm to update membership functions and rule base. Experiments are conducted in the driving simulator under scenarios with different speed levels of lead vehicle: 60 km/h, 80 km/h and 100 km/h. Prediction results show that the proposed method is able to accurately follow steering angle patterns. Furthermore, comparison of prediction performance with several machine learning methods further verifies the learning ability of the AFNN. Finally, a sensibility analysis indicates heading angles and acceleration of vehicle are also important factors for predicting lane changing behavior.</i></p><bibtext xml:space="preserve" id="Esa2018_bib"> @article{TANG2018452, <br />title = "Lane-changes prediction based on adaptive fuzzy neural network", <br /> journal = "Expert Systems with Applications", <br /> volume = "91",<br />pages = "452 - 463",<br />year = "2018",<br />issn = "0957-4174",<br />doi = "https://doi.org/10.1016/j.eswa.2017.09.025",<br /> author = "Jinjun Tang and Fang Liu and Wenhui Zhang and Ruimin Ke and Yajie Zou", <br />}<br /></bibtext></div><script language="JavaScript">hideblock('Esa2018_bib');hideblock('Esa2018_abs');</script></td>
            </tr>
            <tr>
              <td width="20%" valign="top">
                <img src="icons/idea2017.png" style="border-style: none" width="100%" />
              </td>
              <td width="80%" valign="top"><papertitle><a href="./pdfs/idea2017.pdf">Active Safety-Collision Warning Pilot in Washington State </a></papertitle><br />Jerry Spears, Jerome Lutin, Yinhai Wang, <strong><b>Ruimin Ke</b></strong>, Steven Clancy<br />TRANSIT-IDEA Program Project Final Report, 2017<br /><div class="paper" id="Idea2017"><a href="javascript:toggleblock('Idea2017_abs')">abstract</a> / <a href="javascript:toggleblock('Idea2017_bib')">bibtex</a> / <a target="_blank" rel="noopener noreferrer" href="https://trid.trb.org/View/1480393">link</a> / <a target="_blank" rel="noopener noreferrer" href="https://www.kiro7.com/news/local/videos-show-warning-system-helps-bus-drivers-in-close-call-scenarios/276958895">media coverage</a><p align="justify"><i id="Idea2017_abs"> The Rosco/Mobileye Shield+ system is a collision avoidance warning system (CAWS) specifically designed for transit buses. This project involved field testing and evaluation of the CAWS in revenue service over a three-month period. The system provides alerts and warnings to the bus driver for the following conditions that could lead to a collision: 1) changing lanes without activating a turn signal, 2) exceeding posted speed limit, 3) monitoring headway with the vehicle leading the bus, 4) forward vehicle collision warning, and 5) pedestrian or cyclist collision warning in front of, or alongside the bus. Alerts and warnings are displayed to the driver by visual indicators located on the windshield and front pillars. Audible warnings are issued when collisions are imminent. Research objectives included: create a robust Rosco/Mobileye demonstration pilot for active/collision avoidance within the State of Washington on a minimum of 35 transit buses; determine the ease of retrofit of the existing fleet; develop a methodology for estimating the full costs savings of avoided collisions for each agency; develop a methodology and evaluation process for transit driver feedback and acceptance as well as bus passenger feedback; and provide detailed data and understanding on entrance barriers to this technology. The pilot test showed that although driver acceptance was mixed, there were large reductions in near-miss events for CAWS-equipped buses. Consequently, achieving driver acceptance will be a key factor in continued development and deployment of CAWS. As a result of comments received from the drivers, the vendor has begun a program to incorporate desired modifications to the system including reducing false positives. A second major factor in achieving industry acceptance is to demonstrate the business case for CAWS to both transit agencies and system developers. Although the pilot project produced encouraging results, collisions, injuries and fatalities can be considered rare events. A much larger in-service test will be needed to demonstrate actual cost-savings.</i></p><bibtext xml:space="preserve" id="Idea2017_bib"> @Report{01643748, <br />author={Jerry Spears and Jerome Lutin and Yinhai Wang and Ruimin Ke and Steven Clancy},  <br /> journal={Transit IDEA Project}, <br /> title={Active Safety-Collision Warning Pilot in Washington State},<br />year={2017},<br />publisher={Transportation Research Board}<br />volume={}, <br />number={82},<br />pages={1-33}, <br />month={May},}<br /></bibtext></div><script language="JavaScript">hideblock('Idea2017_bib');hideblock('Idea2017_abs');</script></td>			  
            </tr>
            <tr>
              <td width="20%" valign="top">
                <img src="icons/drivenet2016.gif" style="border-style: none" width="100%" />
              </td>
              <td width="80%" valign="top"><papertitle><a href="./pdfs/drivenet2016.pdf">Digital Roadway Interactive Visualization and Evaluation Network Applications to WSDOT Operational Data Usage</a></papertitle><br />Yinhai Wang, <strong><b>Ruimin Ke</b></strong>, Weibin Zhang, Zhiyong Cui, Kristian Henrickson<br />Washington Station Department of Transportation (WSDOT) Research Report, 2016<br /><div class="paper" id="Drivenet2016"><a href="javascript:toggleblock('Drivenet2016_abs')">abstract</a> / <a href="http://www.wsdot.wa.gov/research/reports/800/digital-roadway-interactive-visualization-and-evaluation-network-applications">link</a> / <a target="_blank" rel="noopener noreferrer" href="https://www.youtube.com/watch?v=uzOl3cNoLvc">video</a> / <a target="_blank" rel="noopener noreferrer" href="http://www.uwdrive.net/">website</a><p align="justify"><i id="Drivenet2016_abs">DRIVE Net is a region-wide, Web-based transportation decision support system that adopts digital roadway maps as the base, and provides data layers for integrating and analyzing a variety of data sources (e.g., traffic sensors, incident records). Moreover, DRIVE Net offers a platform for streamlining transportation analysis and decision making, and it serves as a practical tool for visualizing historical observations spatially and temporally. In its current implementation, DRIVE Net demonstrates the potential to be used as a standard tool for incorporating multiple data sets from different fields and as a platform for real-time decision making. In comparison with the previous version, the new DRIVE Net system is now able to handle more complex computational tasks, perform large-scale spatial processing, and support data sharing services to provide a stable and interoperable platform to process, analyze, visualize, and share transportation data. DRIVE Net’s capabilities include generating statistics for WSDOT’s Gray Notebook (GNB), including travel times, throughput productivity, and traffic delay calculations for both general purpose and HOV lanes, each of which are important performance indicators in the WSDOT congestion report. The DRIVE Net system includes robust loop detector data processing and quality control methods to address the data quality issues impacting loop detectors throughout the state. The capabilities of the DRIVE Net system have been expanded to include safety modeling, hotspot identification, and incident induced delay estimation. Specifically, the Safety Performance module includes functions that can be used to obtain traffic incident frequency, apply predictive models to estimate the safety performance of road segments, and visualize and compare observed incident counts and different predictive models. Additionally, a module providing multi-modal data analysis and visualization capabilities was developed as a pilot experiment for integration of heterogeneous data. This module includes pedestrian and bicycle, public transit, park and ride, Car2Go, and ferry data downloading and visualization. DRIVE Net now offers role-based access control, such that access privileges to different functions and data resources can be assigned on a group or individual basis. The new system is able to support more complex analytics and decision support features on a large-scale transportation network, and is expected to be of great practical use for both traffic engineers and researchers. With a modular structure and mature data integration and management framework, DRIVE Net can be expanded in the future to include a variety of additional data resources and analytical capabilities. </i></p> </div><script language="JavaScript"> hideblock('Drivenet2016_abs');</script></td>			 
            </tr>
            <tr>
          </table>
          <br />
        </td>
      </tr>
      <tr>
        <td>
          <heading>Teaching</heading>
          <table cellpadding="10" width="100%" align="left" border="0">
            <tr>
              <td width="100%" valign="top"><strong>SC5302: Smart Cities Design</strong><br />As Instructor / University of Texas at El Paso</a><br/>Spring 2023 </td>
            </tr>
            <tr>
              <td width="100%" valign="top"><strong>CE5390: Special Topics in Civil Engineering - Intelligent Transportation Systems</strong><br />As Instructor / University of Texas at El Paso</a><br/>Fall 2022 </td>
            </tr>
            <tr>
              <td width="100%" valign="top"><strong>CE4340: Transportation Engineering</strong><br />As Instructor / University of Texas at El Paso</a><br/>Spring 2023, Spring 2022 </td>
            </tr>
            <tr>
           <!--   <td width="20%" valign="top">
                <img src="./icons/cv4its.png" style="border-style: none" width="100%" />
              </td> -->
              <td width="100%" valign="top"><strong>Computer Vision for Intelligent Transportation Systems</strong><br />As Invited Lecturer / Connecting with Professionals event for the "18225 High Definition" Robotics Team<br /><a target="_blank" rel="noopener noreferrer" href="https://www.twitch.tv/videos/732545457">Lecture Video</a><br/>Fall 2020 </td>
            </tr>
            <tr>
            <!-- <td width="20%" valign="top">
                <img src="./icons/cet590_1.png" style="border-style: none" width="100%" />
              </td> --> 
              <td width="100%" valign="top"><strong>CET590: Traffic Simulation and System Operations</strong><br />As Pre-Doctoral Instructor / University of Washington<br/>Fall 2019 </td>
            </tr>
          </table>
          <br />
          <br />
        </td>
      </tr>
    <tr>
      <td>
        <heading>Research Group and Opportunities</heading>
        <table cellpadding="10" width="100%" align="left" border="0">
          <tr>
            <td width="100%" valign="top"><strong>Current Group Members</strong><br /> Talha Azfar, PhD student  <a target="_blank" rel="noopener noreferrer" href="https://www.linkedin.com/in/talha-azfar/">LinkedIn</a>  <a target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/citations?user=upSJGWUAAAAJ&hl=en">Google Scholar</a>
          </tr>
          <tr>
            <td width="100%" valign="top"><strong>Prospective Students</strong><br /> I am actively looking for strong and motivated students to join our group! If you are interested in working with me, please email me with you CV, transcripts, GRE score, and any other expectations. You can also directly apply through the <a target="_blank" rel="noopener noreferrer" href="https://cee.rpi.edu/graduate">RPI CEE programs</a> and mention me in your application. I will try my best to respond to every applicant's email.</br> <u><b>RPI students:</u></b> (1) If you are already a graduate student at RPI, feel free to reach out. (2) I am also accepting undergraduate students for individual studies.
          </tr>
        </table>
        <br />
        <br />
      </td>
    </tr>
  </table>
    <script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,'script','//www.google-analytics.com/analytics.js','ga'); ga('create', 'UA-41929264-1', 'berkeley.edu'); ga('send', 'pageview');</script>
  </body>
</html>
